{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e6d746cb",
      "metadata": {
        "id": "e6d746cb"
      },
      "source": [
        "# Demarcation detection\n",
        "### Simplified script for parameter testing\n",
        "\n",
        "##### del Giorgio et al.\n",
        "##### Version 2023\n",
        "\n",
        "Code to detect linear features from satellite imagery.\n",
        "Jupytr notebook formatting provided for case-based parameter fitting."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up:\n",
        "\n",
        "1. Load the necessary libraries\n",
        "2. Define paths\n",
        "3. Load the image properties\n",
        "4. Load images into arrays"
      ],
      "metadata": {
        "id": "5jmRBfVmOjoa"
      },
      "id": "5jmRBfVmOjoa"
    },
    {
      "cell_type": "code",
      "source": [
        "############ SET UP #####################\n",
        "\n",
        "# Load libraries\n",
        "import os\n",
        "from osgeo import gdal\n",
        "from osgeo import gdalconst\n",
        "from osgeo.gdalconst import *\n",
        "import skimage\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "%matplotlib inline\n",
        "\n",
        "from skimage.exposure import rescale_intensity\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import scipy.ndimage\n",
        "from skimage import measure, util\n",
        "from skimage import morphology\n",
        "from skimage.morphology import (erosion, dilation, opening, closing, binary_closing,\n",
        "                                white_tophat,black_tophat, skeletonize, convex_hull_image, rectangle, square, disk,remove_small_holes)\n",
        "from skimage.filters import threshold_otsu, rank, sato, frangi\n",
        "from skimage.filters.rank import enhance_contrast\n",
        "from skimage.util import img_as_ubyte\n",
        "from skimage.exposure import histogram\n",
        "from skimage.color import label2rgb\n",
        "from skimage.measure import label, regionprops, regionprops_table\n",
        "from skimage.segmentation import clear_border\n",
        "\n",
        "import cv2\n",
        "from fil_finder import FilFinder2D\n",
        "import astropy.units as u\n",
        "import time\n",
        "\n",
        "# Define root path\n",
        "rootPath = \"A:/_BioGeo/giorgioo/Analysis_linearFeatures/\"\n",
        "\n",
        "# Load data for the test site\n",
        "site01 = gdal.Open(rootPath + \"TestData/Sentinel2/Sentinel2_2021-07_2021-12_site01.tif\", GA_ReadOnly)\n",
        "\n",
        "# Get image properties\n",
        "gt = site01.GetGeoTransform() #affine transformation\n",
        "pr = site01.GetProjection() #coordinate system (projection)\n",
        "cols = site01.RasterXSize # number of columns of the raster\n",
        "rows = site01.RasterYSize # number of rows\n",
        "bands = site01.RasterCount"
      ],
      "metadata": {
        "id": "aD0ltZdlOsJa"
      },
      "id": "aD0ltZdlOsJa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a46adc60",
      "metadata": {
        "id": "a46adc60"
      },
      "outputs": [],
      "source": [
        "# Define function for comparing array outputs\n",
        "\n",
        "def plot_comparison(original, filtered, filter_name):\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 10), sharex=True,\n",
        "                                   sharey=True)\n",
        "    ax1.imshow(original, cmap=plt.cm.gray)\n",
        "    ax1.set_title('Test 1')\n",
        "    ax1.axis('off')\n",
        "    ax2.imshow(filtered, cmap=plt.cm.gray)\n",
        "    ax2.set_title('Test 2')\n",
        "    ax2.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Read in test array"
      ],
      "metadata": {
        "id": "NnmIQG70PyW8"
      },
      "id": "NnmIQG70PyW8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set test array size and location\n",
        "\n",
        "pixc = 1000\n",
        "pixr = 1000\n",
        "topr = 0\n",
        "topl = 0\n",
        "out_arr = np.zeros((pixr, pixc, bands))\n",
        "\n",
        "# Read in the test array\n",
        "in_arr = site01.GetRasterBand(band).ReadAsArray(topr, topl, pixc, pixr)"
      ],
      "metadata": {
        "id": "DDdL0iIiPR9X"
      },
      "id": "DDdL0iIiPR9X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Linear feature detection function"
      ],
      "metadata": {
        "id": "AEWG1hIyMtGp"
      },
      "id": "AEWG1hIyMtGp"
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to detect linear features\n",
        "\n",
        "def DetectFeatures(col, row, topright, topleft):\n",
        "    # Define root path\n",
        "    rootPath = \"A:/_BioGeo/giorgioo/Analysis_linearFeatures/\"\n",
        "\n",
        "    ######### FUNCTIONS #######################\n",
        "\n",
        "    # Function for comparing images\n",
        "    def plot_comparison(original, filtered, filter_name):\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 10), sharex=True,\n",
        "                                       sharey=True)\n",
        "        ax1.imshow(original, cmap=plt.cm.gray)\n",
        "        ax1.set_title('In array')\n",
        "        ax1.axis('off')\n",
        "        ax2.imshow(filtered, cmap=plt.cm.gray)\n",
        "        ax2.set_title(filter_name)\n",
        "        ax2.axis('off')\n",
        "\n",
        "    # Sato filter function\n",
        "    def Sato(arr):\n",
        "        sato_filt = sato(arr, sigmas=range(3, 5, 20), black_ridges=False, mode='reflect', cval=0)\n",
        "        return sato_filt\n",
        "\n",
        "    # Stretch array functions\n",
        "\n",
        "    def Stretch_02_90(arr):\n",
        "        x = arr\n",
        "        x[np.isnan(x)] = 0\n",
        "        p02, p90 = np.percentile(x, (2, 90))\n",
        "        out = rescale_intensity(x, in_range=(p02, p90), out_range=(0,1))\n",
        "        return out\n",
        "\n",
        "    def Stretch_02_98(arr):\n",
        "        x = arr\n",
        "        x[np.isnan(x)] = 0\n",
        "        p02, p98 = np.percentile(x, (2, 98))\n",
        "        out = rescale_intensity(x, in_range=(p02, p98), out_range=(0,1))\n",
        "        return out\n",
        "\n",
        "    # Hugh line transformation function\n",
        "    def HughLines(arr):\n",
        "        u8 = arr.astype(np.uint8)\n",
        "        line_image = np.copy(arr) * 0  # creating a blank to draw lines on\n",
        "        # Run Hough on edge detected image\n",
        "        # Output \"lines\" is an array containing endpoints of detected line segments\n",
        "        lines = cv2.HoughLinesP(u8, rho, theta, threshold, np.array([]),\n",
        "                        min_line_length, max_line_gap)\n",
        "        for line in lines:\n",
        "            for x1,y1,x2,y2 in line:\n",
        "                cv2.line(line_image,(x1,y1),(x2,y2),(1,0,0),5)\n",
        "        line_eroded = erosion(line_image, footprint=square(3), out=None) # erosion\n",
        "        return(line_eroded)\n",
        "\n",
        "    ### Morphological cleaning\n",
        "\n",
        "    # 1. Function to remove small regions\n",
        "    def RemoveSmall(arr,area):\n",
        "        label_binary = label(arr) # label image regions\n",
        "        regions = regionprops(label_binary) # get regions\n",
        "        table = regionprops_table(label_binary,properties=('label', 'area'),)\n",
        "        condition = (table['area'] > area) # condition of inclusion\n",
        "        # zero out labels not meeting condition\n",
        "        input_labels = table['label']\n",
        "        output_labels = input_labels * condition\n",
        "        filt_label_binary = util.map_array(label_binary, input_labels, output_labels)\n",
        "        # Create small-filtered binary array\n",
        "        removed = np.where(filt_label_binary > 0, 1, 0)\n",
        "        return removed\n",
        "\n",
        "    # 2. Function to remove floating blobs\n",
        "    def RemoveFloatingBlobs(arr,major_axis):\n",
        "        label_binary = label(arr) # label image regions\n",
        "        regions = regionprops(label_binary) # get regions\n",
        "        table = regionprops_table(label_binary,properties=('label','axis_major_length'),)\n",
        "        condition = (table['axis_major_length'] < major_axis)\n",
        "        # zero out labels not meeting condition\n",
        "        input_labels = table['label']\n",
        "        output_labels = input_labels * condition\n",
        "        filt_label_binary = util.map_array(label_binary, input_labels, output_labels)\n",
        "        # Create blob array\n",
        "        more_blobs = np.where(filt_label_binary > 0, 1, 0)\n",
        "        # remove blobs\n",
        "        noblobs = arr - more_blobs\n",
        "        return noblobs\n",
        "\n",
        "    # 3. Function to remove short regions\n",
        "    def RemoveShort(arr,length):\n",
        "        label_binary = label(arr) # label image regions\n",
        "        regions = regionprops(label_binary) # get regions\n",
        "        table = regionprops_table(label_binary,properties=('label', 'axis_major_length'),)\n",
        "        condition = (table['axis_major_length'] > length) # condition of inclusion\n",
        "        # zero out labels not meeting condition\n",
        "        input_labels = table['label']\n",
        "        output_labels = input_labels * condition\n",
        "        filt_label_binary = util.map_array(label_binary, input_labels, output_labels)\n",
        "        # Create small-filtered binary array\n",
        "        long = np.where(filt_label_binary > 0, 1, 0)\n",
        "        return long\n",
        "\n",
        "    ########## PROCESSING ###########\n",
        "\n",
        "    # Assign array dimensions\n",
        "    pixc = col # HOLD CONSTANT\n",
        "    pixr = row # HOLD CONSTANT\n",
        "    topr = topright # MOVING WINDOW - STEP HORIZONTALLY BY 1000\n",
        "    topl = topleft # MOVING WINDOW - STEP VERTICALLY BY 1000\n",
        "\n",
        "    # Select bands\n",
        "    Gp10 = S2.GetRasterBand(7).ReadAsArray(topr, topl, pixc, pixr) # ReadAsArray(0, 0, cols, rows)\n",
        "    Rp10 = S2.GetRasterBand(11).ReadAsArray(topr, topl, pixc, pixr)\n",
        "    NIRmean = S2.GetRasterBand(13).ReadAsArray(topr, topl, pixc, pixr)\n",
        "    NIRp10 = S2.GetRasterBand(15).ReadAsArray(topr, topl, pixc, pixr)\n",
        "    BmeanC = S2.GetRasterBand(17).ReadAsArray(topr, topl, pixc, pixr)\n",
        "    GmeanC = S2.GetRasterBand(18).ReadAsArray(topr, topl, pixc, pixr)\n",
        "    RmeanC = S2.GetRasterBand(19).ReadAsArray(topr, topl, pixc, pixr)\n",
        "    BmeanDIS = S2.GetRasterBand(21).ReadAsArray(topr, topl, pixc, pixr)\n",
        "    GmeanDIS = S2.GetRasterBand(22).ReadAsArray(topr, topl, pixc, pixr)\n",
        "    RmeanDIS = S2.GetRasterBand(23).ReadAsArray(topr, topl, pixc, pixr)\n",
        "\n",
        "    # Rescale\n",
        "    out1 = Stretch_02_90(Gp10)\n",
        "    out2 = Stretch_02_90(Rp10)\n",
        "    out3 = Stretch_02_90(RmeanDIS)\n",
        "    out4 = Stretch_02_98(GmeanDIS)\n",
        "    out5 = Stretch_02_98(BmeanDIS)\n",
        "\n",
        "    # Apply Sato filter\n",
        "    out1_SATO = Sato(out1)\n",
        "    out2_SATO = Sato(out2)\n",
        "    out3_SATO = Sato(out3)\n",
        "    out4_SATO = Sato(out4)\n",
        "    out5_SATO = Sato(out5)\n",
        "\n",
        "    ############ FEATURE DETECTION ####################\n",
        "\n",
        "    #### 1. Detect baseline ####\n",
        "    out3_SATOthresh = np.where(out3_SATO >= 0.07, 1, 0) # Threshold sato output\n",
        "\n",
        "    # Hough line transformation:\n",
        "    rho = 1  # distance resolution in pixels of the Hough grid\n",
        "    theta = np.pi / 350  # angular resolution in radians of the Hough grid\n",
        "    threshold = 100  # minimum number of votes (intersections in Hough grid cell)\n",
        "    min_line_length = 50  # minimum number of pixels making up a line\n",
        "    max_line_gap = 3  # maximum gap in pixels between connectable line segments\n",
        "    line_image = np.copy(out3_SATOthresh) * 0  # creating a blank to draw lines on\n",
        "    line_MAJOR = HughLines(out3_SATOthresh)\n",
        "\n",
        "    #### 2. Refine detection with Random Forest ####\n",
        "\n",
        "    # Array configuration for RFC:\n",
        "\n",
        "    # y processing\n",
        "    MAJ = line_MAJOR # Hugh lines output\n",
        "    maj_y = MAJ.reshape(pixr*pixc) # flatten array\n",
        "    lf_y = maj_y[maj_y==1] # select all demarcations\n",
        "    other_y = np.zeros_like(lf_y) # select all non-demarcations\n",
        "    # x processing\n",
        "    x_arr = np.dstack([out1_SATO, out2_SATO, out3_SATO, out4_SATO, out5_SATO]) # create stacked array with all texture inputs\n",
        "    test = x_arr.reshape(pixr*pixc, x_arr.shape[2]) # flatten 3d array into 2d\n",
        "    xx_lf = test[maj_y==1] # select all pixels that are demarcations\n",
        "    xx_oth_all = test[maj_y==0] # select all pixels that are not demarcations\n",
        "    xx_oth_indices = np.random.choice(xx_oth_all.shape[0], size=other_y.shape[0], replace=False) # get indices for same number of non-demarcation pixels as y\n",
        "    xx_oth = xx_oth_all[xx_oth_indices,:] # select non-demarcation pixels using indice\n",
        "    y_arr_tr = np.concatenate([lf_y, other_y]) # y_train -> target values (demarcations)\n",
        "    x_arr_tr = np.concatenate([xx_lf, xx_oth], axis=0) # X_train -> input sample values (texture)\n",
        "\n",
        "    # Create RF classifier model:\n",
        "    model = RandomForestClassifier(n_estimators=100, #number of trees in the forest. The larger the better, but also the longer it will take to compute.\n",
        "                                   max_features='sqrt', # Size of the random subsets of features to consider when splitting a node.\n",
        "                                                        # The lower the greater the reduction of variance, but also the greater the increase in bias.\n",
        "                                                        # Empirically good default values are max_features=1.0 or equivalently max_features=None (always considering all features instead of a random subset) for regression problems,\n",
        "                                                        # and max_features=\"sqrt\" (using a random subset of size sqrt(n_features)) for classification task (where n_features is the number of features in the data).\n",
        "                                   criterion='gini', # The function to measure the quality of a split\n",
        "                                   max_depth=None, # Use max_depth to control the size of the tree to prevent overfitting\n",
        "                                   min_samples_split=2, # Use min_samples_split or min_samples_leaf to ensure that multiple samples inform every decision in the tree, by controlling which splits will be considered.\n",
        "                                                        # A very small number will usually mean the tree will overfit,\n",
        "                                                        # whereas a large number will prevent the tree from learning the data.\n",
        "                                                        # Try min_samples_leaf=5 as an initial value.\n",
        "                                   min_samples_leaf=1,\n",
        "                                   min_weight_fraction_leaf=0.0, # f the samples are weighted, it will be easier to optimize the tree structure using weight-based pre-pruning criterion such as min_weight_fraction_leaf, which ensure that leaf nodes contain at least a fraction of the overall sum of the sample weights.\n",
        "                                   max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
        "                                   bootstrap=True, oob_score=False, # Parameter cross-validation\n",
        "                                   n_jobs=None, # Parallel construction of the trees and the parallel computation of the predictions\n",
        "                                                # If n_jobs=k then computations are partitioned into k jobs, and run on k cores of the machine.\n",
        "                                                # If n_jobs=-1 then all cores available on the machine are used.\n",
        "                                   random_state=None, # Controls both the randomness of the bootstrapping of the samples used when building trees\n",
        "                                                      # (if bootstrap=True) and the sampling of the features to consider when looking for the best split\n",
        "                                                      # at each node (if max_features < n_features)\n",
        "                                   verbose=0, # Controls the verbosity when fitting and predicting\n",
        "                                   warm_start=False,\n",
        "                                   class_weight=None, # Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant\n",
        "                                                      # In the form {class_label: weight}. If not given, all classes are supposed to have weight one.\n",
        "                                   ccp_alpha=0.0,\n",
        "                                   max_samples=None # Sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree\n",
        "                                  )\n",
        "    model.fit(x_arr_tr, y_arr_tr) # train model\n",
        "    pred = model.predict_proba(test) # apply trained model to test data\n",
        "    # Extract probability scores\n",
        "    prob1 = pred[:, 1]\n",
        "    prob0 = pred[:, 0]\n",
        "    # Re-shape into 2d arrays\n",
        "    prob1_2d = prob1.reshape(pixr,pixc)\n",
        "    prob0_2d = prob0.reshape(pixr,pixc)\n",
        "    # Threshold according to select probability\n",
        "    RF_out = np.where(prob1_2d > 0.5, 1, 0) # np.where(condition[, x, y])  condition : When True, yield x, otherwise yield y.\n",
        "\n",
        "    #### 3. Processing RF output ####\n",
        "\n",
        "    # Morphological processing\n",
        "    closed = closing(RF_out, footprint=square(2), out=None)\n",
        "    RF_sr = RemoveSmall(closed,40) # remove according to area threshold\n",
        "    RF_sr = erosion(RF_sr, footprint=square(3), out=None)\n",
        "    RF_sr = RemoveSmall(RF_sr,50)\n",
        "    RF_sr = RemoveFloatingBlobs(RF_sr,35) # remove according to major axis threshold\n",
        "    filled = remove_small_holes(RF_sr, area_threshold=50, connectivity=0, in_place=False, out=None)\n",
        "    filled = closing(filled, footprint=square(2), out=None)\n",
        "\n",
        "    # 2nd Hough line transformation\n",
        "    rho = 1  # distance resolution in pixels of the Hough grid\n",
        "    theta = np.pi / 350  # angular resolution in radians of the Hough grid\n",
        "    threshold = 45  # minimum number of votes (intersections in Hough grid cell)\n",
        "    min_line_length = 20  # minimum number of pixels making up a line\n",
        "    max_line_gap = 4  # maximum gap in pixels between connectable line segments\n",
        "    line_eroded = HughLines(filled)\n",
        "\n",
        "    # Obtain coded output (Morphological processing round 2)\n",
        "    ALL_clean = RemoveSmall(line_eroded,150) # remove according to area threshold\n",
        "    ALL_clean = closing(ALL_clean, footprint=square(3), out=None)\n",
        "    MAJOR = remove_small_holes(ALL_clean, area_threshold=40, connectivity=0, in_place=False, out=None)\n",
        "    MAJOR_clean = RemoveShort(MAJOR,50) # remove according to length threshold\n",
        "    CODED = MAJOR_clean + ALL_clean\n",
        "    return CODED"
      ],
      "metadata": {
        "id": "7sIKzdxkM-iB"
      },
      "id": "7sIKzdxkM-iB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f28905c0",
      "metadata": {
        "id": "f28905c0"
      },
      "source": [
        "### 3. Apply function to test array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6dc1798",
      "metadata": {
        "id": "d6dc1798"
      },
      "outputs": [],
      "source": [
        "start_time=time.time()\n",
        "\n",
        "# Apply function to test array\n",
        "output = DetectFeatures(pixc,pixr,topr,topl)\n",
        "\n",
        "end_time=time.time()-start_time\n",
        "print(end_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Compare input and output arrays"
      ],
      "metadata": {
        "id": "jR8q65qkMiy2"
      },
      "id": "jR8q65qkMiy2"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_comparison(in_arr, output, Process_name):"
      ],
      "metadata": {
        "id": "s9LGr_LPRUpz"
      },
      "id": "s9LGr_LPRUpz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e73154c3",
      "metadata": {
        "id": "e73154c3"
      },
      "source": [
        "### 5. Write out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b45d97ff",
      "metadata": {
        "scrolled": true,
        "id": "b45d97ff"
      },
      "outputs": [],
      "source": [
        "#Write out output\n",
        "outFolder = 'A:/_BioGeo/giorgioo/Analysis_linearFeatures/Ouput/Sensitivity_analysis/'\n",
        "drvR = gdal.GetDriverByName('GTiff')\n",
        "outRas = drvR.Create(outFolder + 'test.tif', pixc, pixr, 1, GDT_Byte)\n",
        "outRas.SetProjection(pr)\n",
        "\n",
        "print('done')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}